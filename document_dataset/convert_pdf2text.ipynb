{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf3eb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"DentalGPT:AMulti-ExpertTransformerModelforDentalInquiryResolution\\n1\\n2\\n3 DATTRANandVUCAO,ThuyloiUniversity,Vietnam\\n4\\n5 Theproliferationofgeneral-purposelargelanguagemodels(LLMs)hashighlightedasignificantgapinspecializeddomainssuch\\n6 asdentistry,particularlyfornon-Englishlanguages.Existingsystemsoftenlackthedomain-specificknowledgeandcontextual\\n7 understandingrequiredforaccuratemedicalconsultation,especiallyforVietnameseusers.ThispaperintroducesDentalGPT,a\\n8\\nspecializedconversationalagentdesignedtoaddresstheselimitationsbyprovidingpreciseandcontextuallyrelevantdentaladvicein\\n9\\nVietnamese.Thecoreofourmethodologyinvolvesfine-tuningtheDeepSeek-R1model,whichleveragesaMixture-of-Experts(MoE)\\n10\\narchitecture,onacomprehensive,custom-builtVietnamesedentaldataset.Thisdatasetcomprisesapproximately3milliondialogue\\n11\\n12 samplesaggregatedfromdiversesources,includingmedicalliterature,clinicalguidelines,anddoctor-patientconversations,ensuring\\n13 bothprofessionalaccuracyandpracticalrelevance.ThetrainingprocessemployedacombinationofSupervisedFine-Tuning(SFT)\\n14 withQLoRAforefficientoptimizationandReinforcementLearningfromHumanFeedback(RLHF)toalignmodelresponseswith\\n15 expertstandardsanduserexpectations.Quantitativeevaluationsdemonstratethemodel\\u2019shighperformance,achievingaPerplexity\\n16\\nof1.88,aBLEUscoreof0.53,andaBERTScoreof0.93.Incomparativebenchmarks,DentalGPTshowsstate-of-the-artcapabilities,\\n17\\nscoring91.0onMMLU,outperformingprominentmodelslikeGPT-4o.Theresultingsystemisareliableanduser-friendlychatbotthat\\n18\\nsuccessfullybridgesthegapinaccessibledigitalhealthcare,provingtheefficacyoffine-tuningexpert-basedmodelsforspecialized,\\n19\\nnon-Englishdomains.Codeisavailableat:https://github.com/Nvcoing/DentalGPT.git\\n20\\n21\\nCCSConcepts:\\u2022Computingmethodologies\\u2192Artificialintelligence;Naturallanguageprocessing;Languageresources;\\n22\\nNaturallanguagegeneration;Dialoguemanagement;\\u2022Human-centeredcomputing\\u2192Humancomputerinteraction\\n23\\n(HCI);Interactionparadigms;Conversationalinteraction;\\u2022Appliedcomputing\\u2192Healthcareinformationsystems;\\n24\\n25 Consumerhealth..\\n26\\nAdditionalKeyWordsandPhrases:Healthcarechatbot,dentalconsultation,largelanguagemodels(LLMs),mixture-of-experts(MoE),\\n27\\nsupervisedfine-tuning(SFT),reinforcementlearningfromhumanfeedback(RLHF),parameter-efficientfine-tuning(PEFT),vietnamese\\n28\\nNLP\\n29\\n30 ACMReferenceFormat:\\n31\\nDatTranandVuCao.2025.DentalGPT:AMulti-ExpertTransformerModelforDentalInquiryResolution.InProceedingsofMakesure\\n32\\ntoenterthecorrectconferencetitlefromyourrightsconfirmationemail(Conferenceacronym\\u2019XX).ACM,NewYork,NY,USA,16pages.\\n33\\nhttps://doi.org/XXXXXXX.XXXXXXX\\n34\\n35\\n36 1 Introduction\\n37\\nThe rapid advancement of Large Language Models (LLMs) [20] has revolutionized numerous domains, including\\n38\\nhealthcare, by enabling the development of intelligent consultation systems. However, the effectiveness of these\\n39\\n40 general-purposemodelssignificantlydeclineswhenappliedtohighlyspecializedfieldssuchasdentistry,whichdemand\\n41 strict medical accuracy and complex reasoning capabilities. This challenge is further exacerbated in non-English\\n42\\nlanguages,suchasVietnamese,wheredomain-specifictrainingdataisoftenscarce.Asaresult,LLMsmaymisinterpret\\n43\\n44 Authors\\u2019ContactInformation:DatTran,dat.trananh@tlu.edu.vn;VuCao,2151264695@e.tlu.edu.vn,ThuyloiUniversity,Hanoi,Vietnam.\\n45\\nPermissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalorclassroomuseisgrantedwithoutfeeprovidedthatcopiesarenot\\n46\\nmadeordistributedforprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitationonthefirstpage.Copyrightsforcomponents\\n47\\nofthisworkownedbyothersthantheauthor(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,toposton\\n48 serversortoredistributetolists,requirespriorspecificpermissionand/orafee.Requestpermissionsfrompermissions@acm.org.\\n49 \\u00a92025Copyrightheldbytheowner/author(s).PublicationrightslicensedtoACM.\\n50 ManuscriptsubmittedtoACM\\n51\\n52 ManuscriptsubmittedtoACM 1\\n2 DatTranetal.\\n53 contextanddeliverinaccurateorculturallyinappropriateresponses.Thisgaphighlightsacriticalneedforareliable,\\n54 domain-specializedconversationaltooltailoredtothedentalfieldinVietnam.\\n55\\nToaddresstheaforementionedchallenges,thispaperintroducesDentalGPT,adomain-specificconsultationchatbot\\n56\\ndesignedtoprovideaccurate,context-aware,andreliabledentalinformationforVietnameseusers.Thisstudycontributes\\n57\\n58 tothefieldofmedicalnaturallanguageprocessing(NLP)throughthefollowingkeyaspects:(1)First,wepropose\\n59 andimplementanadvancedchatbotarchitecturebasedontheDeepSeek-R1[10]model,whichleveragesaMixture-\\n60\\nof-Experts(MoE)mechanismtohandledental-relatedqueries.ThisrepresentsoneofthefirstapplicationsofMoE\\n61\\narchitecturesinaspecializedhealthcaresystemfortheVietnameselanguage;(2)Second,weconstructandreleasealarge-\\n62\\n63 scaleVietnamesedentaldataset,encompassingdialogues,academicdocuments,andclinicalguidelines,therebyoffering\\n64 avaluableresourceforfutureresearch;and(3)Third,weconductacomprehensiveevaluationprocess,demonstrating\\n65\\nthatourfine-tuneddomain-specificmodeloutperformsgeneral-purposelargelanguagemodelsintasksrelatedto\\n66\\ndentalconsultation.\\n67\\n68 Fromamethodologicalstandpoint,wefine-tunetheDeepSeek-R1modelontheconstructedVietnamesedental\\n69 dataset.Thisprocessconsistsoftwomainstages:(i)SupervisedFine-Tuning(SFT),whichenablesthemodeltolearn\\n70\\nexpert-styledialogueformats;and(ii)ReinforcementLearningfromHumanFeedback(RLHF),whichaimstooptimize\\n71\\nthenaturalness,safety,andreliabilityofthegeneratedresponses.EfficientoptimizationtechniquessuchasQLoRAare\\n72\\n73 alsoemployedtoensurethetrainingprocessremainsfeasibleunderlimitedcomputationalresources.\\n74 Theremainderofthepaperisstructuredasfollows:Section2presentsthetheoreticalbackground,includingthe\\n75\\nTransformerarchitecture,Mixture-of-Experts(MoE)models,andtheDeepSeekframework.Section3detailsthedata\\n76\\ncollectionandpreprocessingprocedures.Section4describesthemodelfine-tuningmethodologyandprovidesan\\n77\\n78 analysisoftheexperimentalresults.Section5discussesthesystemdeploymentanditsevaluationinreal-worldsettings.\\n79 Finally,Section6concludesthepaperandoutlinesdirectionsforfuturework.\\n80\\n81\\n82\\n83\\n84\\n2 Relatedwork\\n85\\n86 2.1 ConversationalAIinHealthcareandSpecializedDomain\\n87\\nOverthepastdecade,conversationalartificialintelligence(ConversationalAI)[2,30]\\u2014particularlychatbotsystems\\u2014has\\n88\\n89 maderemarkableprogressandisincreasinglyappliedinthehealthcaresectortoenhancepatientexperiences.Despite\\n90 thispotential,applyingexistingchatbotsystems[8,40]tothedentaldomainrevealsseveralcriticallimitations.One\\n91\\nmajorissueisthelackofdeepdomain-specificknowledge.Mostchatbots[3,25]arebuiltongeneral-purposedeep\\n92\\nlearningmodelsthatarenotspecializedfordentistry,resultinginlimitedunderstandingofdentalterminologyanda\\n93\\n94 tendencytoprovideinaccurateadvice.Thischallengeisfurthercompoundedbylanguagebarriers,asmanymedical\\n95 chatbotsareprimarilydevelopedinEnglish,limitingtheirabilitytonaturallyprocessandunderstandVietnamese.The\\n96\\nscarcityofVietnamese-languagetrainingdataspecifictodentistrysignificantlyreducestheaccuracyofAImodels\\n97\\ninVietnam.Furthermore,currentsystemsoftenfailtomeettheneedforpersonalization;theytendtooffergeneric\\n98\\n99 responseswithoutaccountingforanindividual\\u2019sdentalhistory.Theselimitationshighlighttheurgentneedfora\\n100 specializedchatbotsystemcapableofdeepcontextualunderstandingandtailoredresponsestomeetthespecificneeds\\n101\\nofusersinthedentaldomain.\\n102\\n103\\n104 ManuscriptsubmittedtoACM\\nDentalGPT:AMulti-ExpertTransformerModelforDentalInquiryResolution 3\\n105 2.2 AdvancementsinTransformer-basedLanguageModels\\n106\\nTheintroductionoftheTransformer[2,28]architecturemarkedagroundbreakingshiftinthefieldofNaturalLanguage\\n107\\n108 Processing(NLP)[28],offeringsubstantialimprovementsoverprevioussequentialmodelssuchasRecurrentNeural\\n109 Networks(RNNs)[6]andLongShort-TermMemory(LSTM)[21]networks.Attheheartofthisinnovationliesthe\\n110 Attentionmechanism\\u2014particularlySelf-Attention\\u2014whichenablesthemodeltoprocesstheentirecontextofasentence\\n111\\ninparallelratherthansequentially.Thisparallelprocessingapproachnotonlyenhancescomputationalefficiency\\n112\\n113 butalsosignificantlyimprovesthemodel\\u2019sabilitytocapturelong-rangesemanticdependencieswithintext.Due\\n114 totheseadvantages,theTransformerhasrapidlybecomethefoundationalarchitecturebehindmostmodernLarge\\n115 LanguageModels(LLMs),includingprominentmodelfamiliessuchasOpenAI\\u2019sGPT[17],MetaAI\\u2019sLLaMA[32],and\\n116\\nDeepSeek[22].ThesemodelshavebeenwidelyadoptedacrossarangeofNLPtasks,frommachinetranslationandtext\\n117\\n118 summarizationtothedevelopmentofincreasinglysophisticatedchatbotsystems.\\n119\\n120\\n2.3 Mixture-of-Experts(MoE)forEfficientModelScaling\\n121\\n122 Toaddressthechallengesofperformanceandcomputationalcostassociatedwithscalinglargelanguagemodels,the\\n123 Mixture-of-Experts(MoE)[4,26,38]architecturehasemergedasaneffectivesolution.Ratherthanrequiringtheentire\\n124\\nneuralnetworktoprocesseveryinput,MoE[38]isadeeplearningarchitecturedesignedtodecomposeataskinto\\n125\\nsmallercomponents,eachhandledbyaspecializedmoduleknownasanexpert.Akeycomponentofthisarchitecture,\\n126\\n127 calledtherouter,analyzestheinputandselectivelyactivatesonlyasubsetofthemostrelevantexpertstoprocessthat\\n128 information.\\n129\\nThismechanismofferstwoprimaryadvantages:itsignificantlyoptimizescomputationalefficiencybyreducing\\n130\\noverallworkloadandtrainingcosts[11],anditenhancesmodelaccuracybyallowingeachexperttofocusdeeplyona\\n131\\n132 specificdomain\\u2014forexample,diseasediagnosis,treatmentconsultation,orplanningtasksinthefieldofdentistry[26].\\n133 ThisarchitectureunderpinstheDeepSeek[22]modelfamilyusedinthisstudy,enablingabalancebetweenexpert-level\\n134\\nperformanceandefficientresourceutilization.\\n135\\n136\\n137 2.4 Fine-tuningandPreferenceOptimizationTechniques\\n138\\nToadaptthemodelforthespecializedtaskofdentalconsultation,anefficientmulti-stagefine-tuningpipelinewas\\n139\\n140 employed.ThefirststageisSupervisedFine-Tuning(SFT)[24,37],inwhichthemodelistrainedonalarge,carefully\\n141 curatedsetofprompt\\u2013completionpairstolearnthestructureandcontentofexpert-styleresponses.Toenablethisprocess\\n142 underconstrainedcomputationalresources,thestudyadoptsQLoRA(QuantizedLow-RankAdaptation)[7],aparameter-\\n143\\nefficientfine-tuning(PEFT)[34]technique.QLoRAcombines4-bitquantizationofmodelweights\\u2014whichsignificantly\\n144\\n145 reducesmemoryusage\\u2014withLow-RankAdaptation(LoRA),whichupdatesonlyasmallsubsetofparameters.This\\n146 allowsthemodeltoretainitsfoundationalknowledgewhileeffectivelyincorporatingnew,domain-specificinformation.\\n147 FollowingSFT[24],themodelisfurtheroptimizedusingReinforcementLearningfromHumanFeedback(RLHF)[1],\\n148\\nspecificallythroughOddsRatioPreferenceOptimization(ORPO)[12].UnliketraditionalRLHFmethodsthatrequirea\\n149\\n150 separaterewardmodel,ORPOdirectlyoptimizesthelog-probabilityofpreferredresponsesovernon-preferredones.\\n151 Thisapproachalignsmodeloutputsmoreeffectivelywithhumanpreferenceswhilebeingmoreresource-efficient.\\n152 ThecombinationofSFTforfoundationalknowledgealignmentandRLHF/ORPOforfine-grainedbehavioraladjust-\\n153\\nmentresultsinarobusttrainingpipelinethatenablesthechatbottobenotonlyfactuallyaccurate,butalsonaturaland\\n154\\n155 trustworthyinitsinteractions.\\n156 ManuscriptsubmittedtoACM\\n4 DatTranetal.\\n157 3 AdaptingMixture-Of-ExpertsforDentalinquiryresolution\\n158\\nWeproposeamulti-stagemethodologyforthedevelopmentofDentalGPT,adomain-specificlanguagemodeldesigned\\n159\\n160 forVietnamesedentalconsultation,asillustratedinFigure4.1.Thismethodologyframeschatbotdevelopmentasa\\n161 taskoffine-tuningandpreferenceoptimization.Theinputstoourapproachinclude:(1)apre-trainedlargelanguage\\n162 model(LLM),specificallyDeepSeek-R1,whichisbuiltonanefficientMixture-of-Experts(MoE)architecture;and(2)a\\n163\\nlarge-scaleVietnamesedentaldataset,consistingofexpert-likeprompt\\u2013responsepairsandhuman-labeledpreference\\n164\\n165 pairs(preferredvs.non-preferredresponses).Thedesiredoutputisafine-tunedmodel,DentalGPT,capableofgenerating\\n166 medicallyaccurate,contextuallyappropriate,andnaturallyphrasedresponses,comparabletothoseofaprofessional\\n167 dentalconsultant.\\n168\\nOurtrainingpipelineconsistsoftwomainstages.Inthefirststage,SupervisedFine-Tuning(SFT),weemploythe\\n169\\n170 parameter-efficientQLoRAtechniquetoinjectdomain-specificknowledgefromthedentaldatasetintothemodel,\\n171 enablingittolearnspecializedformatsandterminology.Inthesecondstage,PreferenceOptimization,weapply\\n172 ReinforcementLearningfromHumanFeedback(RLHF),specificallytheOddsRatioPreferenceOptimization(ORPO)\\n173\\nalgorithm,torefinethemodel\\u2019sbehavior\\u2014ensuringitsresponsesaremorenatural,safe,andalignedwithuserexpecta-\\n174\\n175 tions.\\n176 Theremainderofthissectiondetailseachcomponentoftheproposedmethodology,includingdatapreparation,\\n177 fine-tuningarchitecture,andexperimentalsetup.\\n178\\n179\\n3.1 Networkarchitecture\\n180\\n181 ThenetworkarchitectureofDentalGPTisdesignedfollowingastructuredpipelinecomprisingthreemaincomponents:\\n182\\nDataSource,TrainingTask,andEvaluation,asillustratedinFigure1.TheprocessbeginswiththeselectionofDeepSeek-\\n183\\n184 R1asthefoundationalmodel\\u2014apowerfullargelanguagemodel(LLM).ThismodelisthenadaptedusingQuantized\\n185 Low-RankAdaptation(QLoRA)toenableefficientfine-tuning.Inparallel,thedomain-specificdentaldataisprepared,\\n186 consistingoftwokeyresources:DentalTrainingDataandDentalFeedbackData.Theseresourcesarefedintoatwo-\\n187\\nstagetrainingpipelinetoproducethefinalDentalGPTmodel,whichissubsequentlyevaluatedusingacomprehensive\\n188\\n189 suiteofassessmentmethods.\\n190\\n191 3.1.1 Featureextractionmodule. Thefeatureextractionmoduleinthisarchitectureencompassestheinputcomponents\\n192 responsibleforpreparingfoundationalelementspriortothecoretrainingstages.Attheheartofthismodulelies\\n193\\ntheDeepSeek-R1model,whichservesasapowerfulpre-trainedlanguagefeatureextractor,havingbeentrainedon\\n194\\navastcorpusofdata.Tofine-tuneitsfeatureextractioncapabilitiesinaresource-efficientmanner,theQuantized\\n195\\n196 Low-RankAdaptation(QLoRA)techniqueisapplieddirectlytothemodel.Concurrently,theDentalDataSourceblock\\n197 istaskedwithextractingandsupplyingtwodistinctdatastreams:(1)theDentalTrainingData,comprisinghigh-quality\\n198\\nprompt\\u2013responsepairs,and(2)theDentalFeedbackData,whichcontainshumanevaluationsofresponsequality.The\\n199\\ncombinationofaQLoRA-optimizedmodelandacarefullycurateddomain-specificdatasetestablishesarobustfeature\\n200\\n201 foundationforthesubsequenttrainingstages.\\n202\\n203 3.1.2 Phaseassignmentmodule. Thecoreallocationmodulecorrespondstothetrainingtask,whichisresponsible\\n204\\nfororchestratingandexecutingthetwosequentialfine-tuningstagesrequiredtodevelopthemodel.Thefirststage\\n205\\nisSupervisedFine-Tuning(SFT),inwhichtheDeepSeek-R1model\\u2014alreadyprocessedthroughQLoRA\\u2014istrained\\n206\\n207 ontheDentalTrainingData.Thegoalofthisstageistoenablethemodeltoacquiredomain-specificknowledgeand\\n208 ManuscriptsubmittedtoACM\\nDentalGPT:AMulti-ExpertTransformerModelforDentalInquiryResolution 5\\n209\\n210\\n211\\n212\\n213\\n214\\n215\\n216\\n217\\n218\\n219\\n220\\n221\\n222\\n223\\n224\\n225\\n226\\n227\\n228\\n229\\n230\\n231\\n232\\n233\\n234\\n235\\n236\\n237\\n238\\n239\\n240\\n241 Fig.1. OverviewoftheDentalGPT\\n242\\n243\\n244\\n245 emulatetheresponsestyleofadentalexpert,resultingintheintermediateversion:DentalGPTv0.1.Thesecondstage\\n246 isFeedbackReinforcementTraining,whereDentalGPTv0.1isfurtherfine-tunedusingtheDentalFeedbackData\\n247 tooptimizeitsoutputsbasedonuserpreferencesandexpectations.ThisstageproducesDentalGPTv0.2.Thefinal\\n248\\nDentalGPTmodelistheoutcomeofthistwo-stageprocess,integratingbothexpert-levelknowledgeandhuman-aligned\\n249\\n250 communicationcapabilities.\\n251\\n252 3.2 Modeltraining\\n253\\n254 ThetrainingofDentalGPTisconductedthroughatwo-stage,resource-efficientpipelinedesignedtoadaptapowerful\\n255 MoE-basedLLM\\u2014DeepSeek-R1\\u2014tothespecializedtaskofVietnamesedentalconsultation.Theunderlyingmodel\\n256\\narchitectureleveragestheMixture-of-Experts(MoE)framework,whichsignificantlyreducescomputationaloverhead\\n257\\nwhilemaintaininghighspecializationperformance.Figure2illustratestheinternalmechanismofatypicalMoElayer\\n258\\n259 usedinDeepSeek-R1.Thearchitectureincludestwokeycomponents:theRouterandtheExperts.\\n260 ManuscriptsubmittedtoACM\\n6 DatTranetal.\\n261\\n262\\n263\\n264\\n265\\n266\\n267\\n268\\n269\\n270\\n271\\n272\\n273\\n274\\n275\\n276\\n277\\n278\\n279\\n280\\n281\\n282\\nFig.2. SchematicdiagramoftheMixture-of-Experts(MoE)architectureusedintheDeepSeek-R1backboneofDentalGPT.\\n283\\n284\\n285 Asshowninthefigure,theRouterreceivestheinputtokenrepresentationandemploysaGatingNetworktocalculate\\n286\\naprobabilitydistributionoverallavailableexpertsub-networks.Basedonthisdistribution,therouterselectsthetop-k\\n287\\nmostrelevantexperts(typically2outofN)toprocesseachinput.Onlytheselectedexpertsareactivated,thereby\\n288\\n289 reducingthecomputationalcostcomparedtodenseactivation.\\n290 EachExpertisafeed-forwardneuralnetworktrainedtospecializeinasubsetoftheinputdistribution.Afterthe\\n291\\nselectedexpertsprocesstheinput,theiroutputsareweightedbythegatingscoresandaggregatedbeforepassing\\n292\\nthroughasoftmaxlayertoproducethefinaloutput.Thissparseactivationschemeallowsforscalabilitywhileenabling\\n293\\n294 sub-modulestospecializeintaskssuchassymptominterpretation,treatmentsuggestion,orcontextualunderstanding.\\n295 Themodeltrainingprocessproceedsintwostages:\\n296\\n297 \\u2022 Stage 1: Supervised Fine-Tuning (SFT). The DeepSeek-R1 model is fine-tuned using the Dental Training\\n298 Data,consistingofhigh-qualityprompt\\u2013responsepairscuratedbyexperts.Tooptimizetrainingefficiencyon\\n299\\nlimitedhardware,weapplyQLoRA,whichquantizesmodelweightsto4-bitprecisionandleveragesLow-Rank\\n300\\nAdaptation(LoRA)toupdateonlyafractionofthemodelparameters.Thisstageresultsintheintermediate\\n301\\n302 version:DentalGPTv0.1.\\n303 \\u2022 Stage 2: Preference Optimization via RLHF. In the second stage, we further align the model with human\\n304\\nexpectationsusingReinforcementLearningfromHumanFeedback(RLHF).Specifically,weemploytheOdds\\n305\\nRatioPreferenceOptimization(ORPO)algorithmtorefineresponsequalitybasedoncomparativefeedback\\n306\\n307 providedintheDentalFeedbackData.Thisresultsinthefinalversion:DentalGPTv0.2,whichcombines\\n308 expert-levelaccuracywithnaturalanduser-alignedcommunication.\\n309\\nThis two-stage process, anchored in the MoE architecture, allows DentalGPT to deliver reliable, efficient, and\\n310\\n311 context-awaredentalconsultationservicesinVietnamese.\\n312 ManuscriptsubmittedtoACM\\nDentalGPT:AMulti-ExpertTransformerModelforDentalInquiryResolution 7\\n313 ThetrainingprocessofDentalGPTisdividedintotwomainstages,eachemployingadistinctlossfunctiontofulfill\\n314 separateobjectives:(1)transferringdomain-specificknowledge,and(2)refiningconversationalbehaviortoalignwith\\n315\\nhumanpreferences.\\n316\\nSupervisedFine-Tuning(SFT)\\n317\\n318 Inthefirststage,themodelistrainedonlabeleddentaldatausingSupervisedFine-Tuning(SFT).Thegoalisforthe\\n319 modeltoimitateexpertresponsesbyminimizingtheNegativeLog-LikelihoodLoss(alsoknownasCross-EntropyLoss),\\n320\\ndefinedas:\\n321\\n322 \\ud835\\udc47\\n323 L SFT (\\ud835\\udf03)=\\u2212 \\u2211\\ufe01 log\\ud835\\udc5d \\ud835\\udf03(\\ud835\\udc66 \\ud835\\udc61 |\\ud835\\udc66 <\\ud835\\udc61 ,\\ud835\\udc65) (1)\\n324 \\ud835\\udc61=1\\n325 where:\\n326\\n327 \\u2022 \\ud835\\udf03 arethemodelparameters,\\n328 \\u2022 \\ud835\\udc65 istheinputsequence(userquery),\\n329 \\u2022 \\ud835\\udc66=(\\ud835\\udc66\\n1\\n,\\ud835\\udc66\\n2\\n,...,\\ud835\\udc66 \\ud835\\udc47)isthereferenceoutputsequence(expertanswer),\\n330\\n\\u2022 \\ud835\\udc5d \\ud835\\udf03(\\ud835\\udc66 \\ud835\\udc61 | \\ud835\\udc66 <\\ud835\\udc61 ,\\ud835\\udc65) istheprobabilitypredictedbythemodelfortoken\\ud835\\udc66 \\ud835\\udc61 attimestep\\ud835\\udc61,giventhequery\\ud835\\udc65 and\\n331\\n332 previouslygeneratedtokens\\ud835\\udc66 <\\ud835\\udc61.\\n333 Byminimizingthisloss,themodellearnstogeneratehigh-probabilitysequencesthatcloselyresembleexpert-crafted\\n334\\nresponsesinthetrainingdataset.\\n335\\nPreferenceOptimizationwithORPO\\n336\\n337 AfterestablishingaknowledgebasethroughSFT,themodelisfurtherrefinedtoproducemorenaturalanduser-aligned\\n338 responsesusingOddsRatioPreferenceOptimization(ORPO).ORPOisanefficientmethodthatdoesnotrequirea\\n339\\nseparaterewardmodel.Instead,itoptimizesdirectlyoverpairsofresponses\\u2014onepreferred(\\ud835\\udc66 \\ud835\\udc64)andonelesspreferred\\n340\\n341\\n(\\ud835\\udc66 \\ud835\\udc59)\\u2014forthesameprompt\\ud835\\udc65.ThecorelossfunctionofORPOisbasedonthelogoddsratio:\\n342\\n3 3 4 4 3 4 L ORPO (\\ud835\\udf03)=\\u2212log\\ud835\\udf0e (cid:18) log \\ud835\\udc5d \\ud835\\udc5d \\ud835\\udf03 \\ud835\\udf03 ( ( \\ud835\\udc66 \\ud835\\udc66 \\ud835\\udc64 \\ud835\\udc59 | | \\ud835\\udc65 \\ud835\\udc65 ) )(cid:19) (2)\\n345 where:\\n346 \\u2022 \\ud835\\udc66 \\ud835\\udc64 (winner)isthepreferredresponse,\\n347\\n348 \\u2022 \\ud835\\udc66 \\ud835\\udc59 (loser)isthelesspreferredresponse,\\n349 \\u2022 \\ud835\\udc5d \\ud835\\udf03(\\ud835\\udc66 |\\ud835\\udc65)istheprobabilityassignedbythemodeltoresponse\\ud835\\udc66giveninput\\ud835\\udc65,\\n350 \\u2022 \\ud835\\udf0eisthesigmoidfunction.\\n351\\n352 Minimizingthislossencouragesthemodeltoincreasethelikelihoodofgenerating\\ud835\\udc66 \\ud835\\udc64 whiledecreasingthatof\\ud835\\udc66 \\ud835\\udc59,\\n353 therebyaligningthechatbot\\u2019sresponsesmorecloselywithhumanpreferences.\\n354\\n355 3.3 Intuitionofourapproach\\n356\\nTheintuitionbehindourapproachistodevelopachatbotsystemthatisnotonlyacademicallyrobustbutalsopractical\\n357\\n358 anddeployment-readyforreal-worldapplications.Toachievethis,wedesignedamodernthree-tieredarchitecture\\n359 thatclearlyseparatestheuserinterface,backendlogic,andlanguagemodelcore,asillustratedinFigure3.Userscan\\n360\\ninteractwiththesystemnaturallythroughanintuitiveuserinterfaceaccessibleacrossvariousdevices,rangingfrom\\n361\\ndesktopcomputerstosmartphones.AlluserrequestsaresecurelytransmittedtothebackendsystemviaPOSTAPI[35]\\n362\\n363 calls,awidelyadoptedstandardinmodernwebapplications.\\n364 ManuscriptsubmittedtoACM\\n8 DatTranetal.\\n365\\n366\\n367\\n368\\n369\\n370\\n371\\n372\\n373\\n374\\n375\\n376\\n377\\n378 Fig.3. OverviewofDentalGPTSystemArchitecture\\n379\\n380\\n381\\nAtthecoreofthesystemliesthebackend,whereaFastAPI-basedbackendserverfunctionsasthecentralorchestrator.\\n382\\n383 Ratherthansimplyforwardingrequeststothelanguagemodel,thisserveractsasanintelligentcontrollercapableof\\n384 determiningthemostoptimalwaytoprocesseachquery.Itinteractswithtwomaincomponents:theLLMServer,which\\n385\\nhoststhefine-tunedDentalGPTmodelresponsibleforinferenceandtextgenerationtasks,andtheToolCallmodule.\\n386\\nTheToolCallmoduleenablesthesystemtoextenditscapabilitiesbyinvokingexternaltools,suchassearchAPIs,\\n387\\n388 medicaldatabases,oranalyticalservices.Thisprocessingflowallowsthesystemtogobeyondtheinternalknowledge\\n389 oftheLLM,retrievingup-to-dateinformationorexecutingcomplextasks,therebygeneratingresponsesthataremore\\n390\\naccurate,current,andusefultotheendusers.\\n391\\n392\\n393 4 ViDentalDataset\\n394\\nOurobjectiveistodevelopahigh-accuracy,context-awaredentalconsultationchatbottailoredspecificallyforViet-\\n395\\nnameseusers.However,existingdatasetsarenotsuitableforthispurpose,astheyareprimarilydevelopedinEnglish,\\n396\\n397 lackdomain-specificdepthindentistry,orarenotoptimizedfortheculturalandhealthcarepracticesinVietnam.This\\n398 gaphasmotivatedustoconstructandannotateanewdataset,whichwenametheViDentalDataset.\\n399\\n400\\n4.1 DataCollection\\n401\\n402 Buildingahigh-quality,diverse,anddomain-specificdatasetisafoundationalstepthatcriticallydeterminesthefinal\\n403\\nperformanceoftheDentalGPTmodel.Toensurethedatasetmeetstherigorousdemandsofamedicalconsultation\\n404\\n405 chatbot,weadoptedamulti-sourcedatacollectionstrategydesignedtooptimizethreecoreaspects:domainspecificity\\n406 indentistry,contextualdiversity,andinformationfreshness.Thisprocesswasnotmerelyamatterofdataaggregation,\\n407 butastructuredknowledge-processingpipelinethatintegratescontentfromopendataplatforms,academicrepositories,\\n408\\nandunstructuredwebsources.\\n409\\n410 Thefirststepinourpipelineinvolvedmininglarge-scalepublicdatasetsfromplatformssuchasHuggingFace[29],\\n411 Kaggle,andGoogleDatasetSearch.Whiletheserepositoriescontainwidelyusedandvalidateddatasets,theyareoften\\n412 notoptimizedforthedentaldomaininVietnam.Toaddressthis,weappliedkeyword-basedfiltersanddomain-specific\\n413\\ncriteriatocarefullyextractandcuraterelevantdental-relatedcontent,minimizingnoiseandensuringhightopicalfocus\\n414\\n415 inthefinaldataset.\\n416 ManuscriptsubmittedtoACM\\nDentalGPT:AMulti-ExpertTransformerModelforDentalInquiryResolution 9\\n417 Inparallel,weconducteddeepknowledgeextractionfromacademicplatformslikearXivandResearchGateusing\\n418 toolssuchaswebscrapersandAPIs(e.g.,thearXivAPIandGoogleSearchAPI).Extractedcontentincludedtitles,\\n419\\nabstracts,andfull-textresearcharticlesinformatslikePDForDOCX,whichwerethenconvertedtoplaintextusing\\n420\\nlibrariessuchasPyMuPDF[33].Incorporatingscholarlydataenrichedthemodel\\u2019sknowledgebasewithverified\\n421\\n422 professionalinsights,therebyimprovingboththeexplainabilityandcredibilityofitsresponses.\\n423 Toenablethemodeltobettercapturenaturaluserinteractions,wealsogathereddatafromunstructuredwebsources,\\n424\\nincludingdental-specificwebsites,dentists\\u2019blogs,andonlineforumsonoralhealthQ&A.Aftercarefulprocessingand\\n425\\nselection,thisdataprovidedarichsourceofreal-worldcontextualinformation,whichisessentialformodelingnatural\\n426\\n427 dialoguesbetweenusersandthesystem.Throughthismeticulousanddomain-awaredataacquisitionstrategy,the\\n428 ViDentalDatasetcombinestheacademicrigorofverifiedknowledgewiththepracticalrelevanceofcommondental\\n429\\nconcerns\\u2014frombothexpertandpatientperspectives\\u2014formingarobustfoundationfordownstreammodelfine-tuning.\\n430\\n431\\n4.2 DatasetStatistics\\n432\\n433 AdetailedquantitativeanalysiswasconductedtoevaluatethescaleandlinguisticcharacteristicsoftheViDental\\n434\\ndataset.Thisanalysisplaysacrucialroleinunderstandingtheattributesoftheinputdata,whichinturnformsthe\\n435\\nbasisfordesigningandtraininganeffectivechatbotmodel.Keymetricsmeasuredincludethetotalnumberofsamples,\\n436\\n437 averagewordcount,totalwordcount,vocabularydiversity,andsentencelengthdistribution.Theseindicatorsnot\\n438 onlyreflectthedataset\\u2019soverallscalebutalsohighlightitslinguisticrichnessandcomplexity\\u2014essentialfortraining\\n439\\namodelcapableofdeepunderstanding.Table1belowsummarizesthekeyquantitativestatisticsofthedatasetafter\\n440\\npreprocessingandaugmentation.\\n441\\n442\\nTable1. QuantitativeStatisticsoftheViDentalTrainingDataset\\n443\\n444\\n445 EvaluationMetric Value\\n446\\nNumberofdatasamples 3,090,600\\n447\\nAveragenumberofwordsperline 798\\n448\\nTotalwordcount 2,466,298,800\\n449\\nVocabularysize 42,384,200\\n450\\nSentencelengthrange(perline) 18\\u20131,000words\\n451\\n452\\n453 Togainamoreintuitiveunderstandingofthelinguisticcontextderivedfromuserqueries,aWordCloudvisualization\\n454 wasgeneratedfromthe\\u201cQuestion\\u201dfieldofthetrainingdataset(illustratedinFigure3.2).Thisvisualizationoffersan\\n455\\noverviewoftherelativefrequencyofkeyterms,highlightingtheprimaryconcernsofdentalpatients.Phrasessuchas\\n456\\n\\u201coralhealth,\\u201d\\u201cbrushingteeth,\\u201d\\u201ccost,\\u201d\\u201cwhileeating,\\u201dand\\u201cperiodontaldisease\\u201dreflectcommonprioritiesandanxieties\\n457\\n458 expressedbyuserswheninteractingwithdentalservices.Thisanalysisnotonlyhelpsidentifydominanttopicsbut\\n459 alsoprovidesinsightsforimplementingdatabalancingorenrichmentstrategies,enablingthemodeltogeneralizemore\\n460\\neffectivelyacrossreal-worldscenarios.\\n461\\n462\\n5 Experiments\\n463\\n464 5.1 Implementationdetails\\n465\\nOurexperimentswereconductedinaPython-baseddevelopmentenvironmentutilizingflexibleplatformssuchas\\n466\\n467 JupyterNotebook,GoogleColab,andKaggle.Thebasemodelselectedforfine-tuningwasDeepSeek-R1-Distill-LLaMA-8B,\\n468 ManuscriptsubmittedtoACM\\n10 DatTranetal.\\n469 adistilledandcomputationallyefficientvariantsuitablefordeploymentonGPUswith10\\u201316GBofVRAM.Theentire\\n470 trainingandfine-tuningpipelinewasimplementedusingstate-of-the-artopen-sourcelibraries,primarilytheTRL\\n471\\n(TransformersReinforcementLearning)librarybyHuggingFaceforsupervisedfine-tuning(SFT)andpreference\\n472\\noptimization(ORPO),andtheUnslothlibraryforenhancedmemoryefficiencyandacceleratedcomputationthrough\\n473\\n474 optimizedattentionmechanisms.\\n475 We employed the QLoRA (Quantized Low-Rank Adaptation) fine-tuning method, loading the model in a 4-bit\\n476 quantizedformat.TheLoRAconfigurationwassetwitharankof\\ud835\\udc5f =64and\\ud835\\udefc =128,targetingcriticalTransformer\\n477\\ncomponentsincludingq_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,anddown_proj.\\n478\\n479 Thetrainingprocesswasdividedintotwodistinctstages,eachwithcustomizedhyperparameters.IntheSFTstage,\\n480 weusedalearningrateof2\\u00d710\\u22124,aneffectivebatchsizeof200samplesperupdatestep(withaper-devicebatch\\n481\\nsizeof8andgradientaccumulationstepsof4),andtrainedfor2epochs.FortheORPOstage,ahigherlearningrateof\\n482\\n3\\u00d710\\u22124wasadopted,andtrainingwasconductedfor5epochs.TofurtheroptimizeGPUmemoryusage,weemployed\\n483\\n484 the8-bitAdamWoptimizer.Themaximuminputsequencelengthwaslimitedto1024tokenstoaccommodateextended\\n485 conversationalcontexts.\\n486\\nIntermsofinfrastructure,theexperimentswererunacrossacombinationoflocalandcloud-basedhardware.Local\\n487\\nexperimentswereexecutedonapersonalcomputerequippedwithanRTX3050GPU(8GBVRAM),whilecloud\\n488\\n489 experimentsutilizedGoogleColab(TeslaT4,16GBVRAM)andKaggle(TeslaP100,16GBVRAM).Theactualtraining\\n490 processrequiredapproximately10GBofCPURAMand12GBofGPUVRAM.Toenhancesystemfunctionality,external\\n491\\nserviceswerealsointegrated,includingtheHuggingFaceAPIformodelanddatasetaccess,Qdrantasavectordatabase\\n492\\nforsemanticsearch,andtheGeminiAPIforauxiliarytaskssuchastranslationanddatageneration.\\n493\\n494\\n495\\n496\\n5.2 TrainingDynamicsandMetricEvolution\\n497\\n498 Toensuretheeffectivenessandstabilityofthefine-tuningprocess,wecloselymonitoredtheprogressionofkey\\n499\\nevaluationmetricsthroughout5,000trainingsteps.Figure4illustratestheevolutionoffluencyandsemanticquality\\n500\\nmetrics,includingPerplexity,BLEU,METEOR,andBERTScore.Thesemetricsoffervaluableinsightsintothemodel\\u2019s\\n501\\n502 learningdynamicsduringsupervisedfine-tuningontheViDentaldataset.\\n503 AsshowninFigure4,themodelexhibitedhighlyfavorablelearningbehavior.Theperplexitycurvedropsrapidly\\n504\\nandsharplyduringtheinitialtrainingstepsandstabilizesatanotablylowvalue(below2.0)afterapproximately3,000\\n505\\nsteps.Thisindicatesthatthemodelquicklyinternalizedlinguisticstructuresanddomain-specificpatterns,becoming\\n506\\n507 increasinglyconfidentinitstokenpredictions.\\n508 Meanwhile,semanticqualitymetricsalsoimprovedconsistently.TheBLEUscore,whichreflectssyntacticsimilarity,\\n509\\nincreasedsteadilyfromaninitialvaluearound0.1tonearly0.5bytheendoftraining.Similarly,METEOR\\u2014more\\n510\\nsensitivetosynonymsandwordorder\\u2014rosetoapproximately0.6,demonstratingenhancedcapabilityinproducing\\n511\\n512 structurallyandlexicallyappropriateoutputs.\\n513 Mostnotably,theBERTScore,adeepsemanticsimilaritymetric,showedastrongandcontinuousupwardtrend,\\n514\\nreachingover0.9inthefinalstagesoftraining.Thishighscoresuggeststhatthemodelwasnotmerelymemorizing\\n515\\nsentencepatternsbutgenuinelygraspingthecontextualandsemanticdepthofthedentaldomain.\\n516\\n517 Overall,thesetrendsconfirmthatthefine-tuningprocesssuccessfullyinstilleddomain-specificknowledgeintothe\\n518 model,yieldingoutputsthatarebothfluentandsemanticallyalignedwithexpert-levelexpectations.\\n519\\n520 ManuscriptsubmittedtoACM\\nDentalGPT:AMulti-ExpertTransformerModelforDentalInquiryResolution 11\\n521\\n522\\n523\\n524\\n525\\n526\\n527\\n528\\n529\\n530\\n531\\n532\\n533\\n534\\n535\\n536\\n537\\n538\\n539\\n540 Fig.4. Evolutionofsemanticandfluencymetricsacross5,000trainingsteps.\\n541\\n542\\n543 5.3 Experimentalresultsandanalysis\\n544\\nToassesstheeffectivenessofthefine-tunedDentalGPTmodel,weconductedacomprehensiveevaluationprocess,\\n545\\ncombiningquantitativemetrics,qualitativeuserfeedback,scenario-basedassessments,andbenchmarkingagainstbase\\n546\\n547 models.Thisprocesswasperformedonatestsetcomprising24,150augmentedsamplestoensureanobjectiveand\\n548 multi-dimensionalevaluation.\\n549\\n550 5.3.1 QuantitativeEvaluation. Thequantitativeevaluationfocusedonseveralcoreaspectsoftextgenerationquality,\\n551\\nincludingfluency,coherence,andsemanticsimilarity.Table2summarizesthekeymetricsandprovidesinterpretations\\n552\\nrelevanttothecontextofamedicalconsultationchatbot.\\n553\\n554\\nTable2. SummaryofQuantitativeEvaluationMetrics\\n555\\n556\\n557 Metric Value Interpretation\\n558\\nPerplexity 1.8824 Highfluencyandlanguagequality\\n559\\nBLEU[31] 0.5251 Syntacticallyaccurateresponses\\n560\\nROUGE-1[27] 0.8422 Goodcaptureofkeyterms\\n561\\nROUGE-2[19] 0.7758 Highphrase-levelsimilarity\\n562\\nROUGE-L[14] 0.6925 Strongsentencecoherence\\n563\\nROUGE-Lsum 0.8393 Outputsummarizeskeypointswell\\n564\\nMETEOR[18] 0.6399 Strongsemanticexpressiveness\\n565\\nBERTScore(F1)[9] 0.9307 Near-human-levelsemanticsimilarity\\n566\\n567\\n568\\nAsshowninTable2,themodeldemonstratesstrongperformance.ThelowPerplexityscoreof1.8824indicates\\n569\\nexcellentlanguagegenerationcapabilities.ThehighROUGEscoresconfirmthemodel\\u2019sabilitytoreconstructkey\\n570\\n571 termsandmaintainstructuralcoherence.Toassessdeepersemanticunderstanding,weappliedadvancedmetricssuch\\n572 ManuscriptsubmittedtoACM\\n12 DatTranetal.\\n573 asBERTScore,whichyieldedanimpressiveF1scoreof0.9307\\u2014suggestingthatDentalGPT\\u2019sresponsesarenearly\\n574 semanticallyequivalenttoexpert-providedreferences,acriticalrequirementformedicalapplications.\\n575\\n576\\n5.3.2 QualitativeandScenario-BasedAssessment. Inadditiontoquantitativemetrics,weconductedaqualitativestudy\\n577\\n578 involving15participantsacrossfivegroups(dentists,healthcareprofessionals,patients,generalusers,andstudents)to\\n579 gatherreal-worldfeedback.Participantsreportedhighsatisfaction,withaveragescoresof8/10forInformationAccuracy,\\n580\\n9/10forClarity,and9/10forSafety.Expertspraisedthemodel\\u2019saccuracy,particularlyinpost-treatmentcareand\\n581\\npreventiveadvice.Generalusersappreciatedtheclearandaccessiblelanguage.\\n582\\n583 Inscenario-basedtests,themodelperformedstronglyinPatientCareAdvice(9/10)andSafetyCompliance(9/10).\\n584 Itconsistentlyprioritizedusersafetybyrecommendingin-personvisitsforacutesymptomsratherthanriskyself-\\n585\\ntreatmentadvice.However,themodelshowedsomelimitationsinEmergencyHandling (7/10)andinrecognizing\\n586\\ncomplexMedicalTerminology(7/10),highlightingareasforfutureenhancement.\\n587\\n588\\n589 5.3.3 BenchmarkingAgainstLeadingLanguageModels. TocontextualizetheperformanceofDentalGPTwithinthe\\n590 broader landscape of state-of-the-art language models, we conducted a comparative analysis using the standard\\n591\\nbenchmarkspublishedintheoriginalDeepSeek-R1study.Table3presentsadirectcomparisonbetweenDentalGPT\\n592\\nandprominentmodelssuchasClaude-3.5,GPT-4o,andvariousDeepSeekvariantsacrossarangeofevaluationtasks,\\n593\\n594 frommulti-domainknowledge(MMLU)todeepreasoning(GPQA,MATH).\\n595\\n596\\n597\\nTable3. ComparisonofDentalGPTagainstoriginalstudymodelsonkeybenchmarks\\n598\\n599\\nBenchmark Claude-3.5 GPT-4o[15] DeepSeekV3[39] o1-mini[16] o1-1217 DeepSeekR1[10] DentalGPT(0urs)\\n600\\nMMLU(Pass@1)[5] 88.3 87.2 88.5 85.2 91.8 90.8 91.0\\n601\\nMMLU-Redux(EM) 88.9 88.0 89.1 86.7 - 92.9 93.2\\n602 MMLU-Pro(EM)[36] 78.0 72.6 75.9 80.3 - 84.0 83.8\\n603 DROP(3-shotF1) 88.3 83.7 91.6 83.9 90.2 92.2 93.4\\nIF-Eval(PromptStrict) 86.5 84.3 86.1 84.8 - 83.3 85.0\\n604\\nGPQADiamond(Pass@1)[13] 65.0 49.9 59.1 60.0 75.7 71.5 73.0\\n605 MATH-500(Pass@1)[23] 78.3 74.6 90.2 90.0 96.4 97.3 91.0\\n606\\n607\\n608\\n609 AsshowninTable3,DentalGPTachievedhighlycompetitiveresults.Onthemulti-domainbenchmarkMMLU,it\\n610 reachedaPass@1scoreof91.0,outperformingitsbasemodelDeepSeek-R1(90.8)andsurpassinglargercommercial\\n611\\nmodelslikeGPT-4o(87.2)andClaude-3.5(88.3).Thisresultstronglysupportstheideathatdomain-specificfine-tuning\\n612\\nonhigh-qualitydatacanenablesmallermodelstosurpassmuchlargeronesinspecializedtasks.\\n613\\n614 ForcomplexreasoningandinformationextractiontaskssuchasDROPandGPQADiamond,DentalGPTachievedF1\\n615 andPass@1scoresof93.4and73.0,respectively.Thelatterresultisparticularlynoteworthy,demonstratingexpert-level\\n616\\nreasoningcapabilitiesdespiteDentalGPThavingonly168milliontrainableparameters.\\n617\\nInotherdomain-specificevaluations,DentalGPTachievedanaverageperformancescoreof 8.49,evenhigherthan\\n618\\n619 theDeepSeekLLM67BChatmodel(8.35),withexceptionalstrengthinInformationExtraction(9.6)andRole-playing\\n620 (9.5).Overall,thesecomparativeresultsconfirmthatDentalGPT\\u2019sthreemostprominentadvantagesarereasoning\\n621\\nability,accurateinformationretrieval,anddomain-appropriatecommunication\\u2014alldirectoutcomesofitstailored\\n622\\nfine-tuningstrategyandcarefullycurateddataset.\\n623\\n624 ManuscriptsubmittedtoACM\\nDentalGPT:AMulti-ExpertTransformerModelforDentalInquiryResolution 13\\n625 5.4 Ablationstudy\\n626\\nToanalyzethecontributionofeachcorecomponentinourmethodology,weconductedanablationstudyfocusingon\\n627\\n628 twokeyelements:(1)theimpactofspecializedfine-tuningontheViDentaldataset,and(2)theeffectivenessofthe\\n629 two-stagetrainingprocess(SupervisedFine-TuningandReinforcementLearningfromHumanFeedback).\\n630\\n631 5.4.1 ImpactofSpecializedFine-TuningontheViDentalDataset. Toassessthevalueofdomain-specificfine-tuning,\\n632 wecomparedtheperformanceofthefullytrainedDentalGPTmodelwithseveralsignificantlylargergeneral-purpose\\n633\\nLLMs,includingGPT-4oandClaude-3.5.Thiscomparisoncanbeinterpretedasanablationstudyofthefine-tuning\\n634\\nprocess,wheretheunmodifiedbasemodelsserveasacontrolcondition.Table4summarizestheresultsacrosskey\\n635\\n636 benchmarkdatasets.\\n637\\nTable4. PerformanceComparisontoEvaluatetheImpactofViDentalFine-Tuning\\n638\\n639\\n640 Benchmark Claude-3.5 GPT-4o DeepSeekV3 o1-mini o1-1217 DeepSeekR1 DentalGPT\\n641\\nMMLU(Pass@1) 88.3 87.2 88.5 85.2 91.3 90.2 91.5\\n642 GPQADiamond(Pass@1) 64.8 49.9 59.1 60.0 75.7 71.2 76.5\\n643 MATH-500(Pass@1) 78.3 74.6 90.2 90.0 96.4 97.3 90.8\\n644\\n645\\n646 AsshowninTable4,despitebeingsignificantlysmallerinsize,DentalGPTachievedcompetitiveandoftensuperior\\n647 results.OntheMMLUbenchmark,DentalGPTreachedaPass@1accuracyof91.2,reflectingstronggeneralknowl-\\n648\\nedgeretention.Mostnotably,ontheGPQADiamondbenchmark\\u2014designedtotestdeepreasoningandexpert-level\\n649\\nunderstanding\\u2014DentalGPTscored74.5,surpassingmostgeneral-purposemodels.\\n650\\n651 Thesestrongresultsunderscorethepowerofhigh-qualitydomain-specificfine-tuning.Ratherthanrelyingsolelyon\\n652 scale,DentalGPTleveragesspecializedtrainingtoacquirecomplexreasoningabilitiesandexpert-levelknowledgein\\n653\\nthedentalfield\\u2014capabilitiesthatgenera\\n654\\n655\\n5.4.2 ContributionoftheTwo-StageTrainingPipeline(SFT+RLHF). Ourtrainingstrategyusesatwo-stagepipeline:\\n656\\nSupervisedFine-Tuning(SFT)totransferdomainknowledge,followedbyReinforcementLearningfromHumanFeedback\\n657\\n658 (RLHF)forbehavioralignment.Toevaluatethecontributionofthesecondstage,weconceptuallyconsideramodel\\n659 trainedonlywithSFT\\u2014effectively\\u201cablating\\u201dRLHF.Theimpactofthisalignmentphaseisprimarilyreflectedinuser\\n660\\ninteractionandsafety,summarizedinTable5.\\n661\\n662\\nTable5. UserFeedbackSummaryReflectingRLHFImpact\\n663\\n664\\nEvaluationCriterion Avg.Score(outof10)\\n665\\n666 InformationAccuracy 8\\n667 Comprehensibility 9\\n668 Interactivity 7\\n669 Decision-MakingSupport 8\\n670 Safety 9\\n671 DomainKnowledge 7\\n672\\n673\\nTheSFTstageprovidesastrongfoundationindomainknowledge,evidentinscoresforInformationAccuracy(8/10)\\n674\\n675 andDomainKnowledge(7/10).However,RLHFcontributessignificantlytothemodel\\u2019shuman-alignedbehavior.As\\n676 ManuscriptsubmittedtoACM\\n14 DatTranetal.\\n677 showninTable5,DentalGPTreceiveshighscoresforComprehensibility(9/10)andSafety(9/10),attributesessential\\n678 foratrustworthymedicalassistant.Thesecapabilities\\u2014suchasprovidingsafeadviceratherthanriskyself-treatment\\n679\\nsuggestions\\u2014stemdirectlyfromalignmentthroughRLHF.\\n680\\nInconclusion,omittingtheRLHFstagewouldresultinamodelthat,whilefactuallyknowledgeable,lacksthe\\n681\\n682 naturalness,safety,andreliabilityrequiredformedicalassistantapplications.Thisconfirmstheessentialroleofour\\n683 two-stagetrainingdesign.\\n684\\n685\\n6 Conclusion\\n686\\n687 Inthiswork,weintroducedDentalGPT,alightweightyethigh-performingdomain-specificlanguagemodeltailored\\n688\\nforthedentalfield.Despiteitsmodestscaleofonly168milliontrainableparameters,DentalGPTdemonstratesstrong\\n689\\ncapabilitiesinmedicalreasoning,informationextraction,anddomain-specificdialogue.Throughacarefullyconstructed\\n690\\n691 two-stagetrainingpipeline\\u2014combiningsupervisedfine-tuningontheViDentaldatasetwithreinforcementlearning\\n692 fromhumanfeedback(RLHF)\\u2014themodelnotonlyacquiresexpert-levelknowledgebutalsoalignseffectivelywithuser\\n693\\npreferencesandsafetyrequirements.\\n694\\nComprehensivebenchmarkevaluationsagainststate-of-the-artgeneral-purposeLLMssuchasGPT-4oandClaude-3.5\\n695\\n696 showthatDentalGPTconsistentlyachievescompetitiveorsuperiorperformanceacrossarangeofreasoningand\\n697 knowledge-intensivetasks.Ourablationstudyfurtherhighlightstheessentialrolesofbothdomain-specificfine-tuning\\n698\\nandbehavioralignmentinachievingtheseresults.\\n699\\nOverall,DentalGPTexemplifieshowtargetedadaptationandsafetyalignmentcanempowersmallermodelstoexcel\\n700\\n701 inspecializeddomains.Futureworkwillexplorebroadermultilingualsupport,integrationwithdentalimagingdata,\\n702 andreal-worlddeploymentinclinicaldecisionsupportsystems.\\n703\\n704\\nReferences\\n705\\n706 [1] AbdulqaharMukhtarAbubakar,DeepaGupta,andShantipriyaParida.2024.Areinforcementlearningapproachforintelligentconversational\\n707 chatbotforenhancingmentalhealththerapy.ProcediaComputerScience235(2024),916\\u2013925.\\n708 [2] MuzamilAhmed,HikmatUllahKhan,andEhsanUllahMunir.2023.ConversationalAI:anexplicationoffew-shotlearningproblemintransformers-\\n709 basedchatbotsystems.IEEETransactionsonComputationalSocialSystems11,2(2023),1888\\u20131906.\\n[3] DiuliaPereiraBubna,PedroFelipedeJesusFreitas,AlineXavierFerraz,AllanAbuabara,FlaresBaratto-Filho,BiancaMarquesdeMattosdeAraujo,\\n710\\nErikaCalvanoKuchler,LilianeRoskamp,AngelaGracielaDeligaSchroder,andCristianoMirandadeAraujo.2025.DentalTraumaEvo\\u2013Development\\n711\\nofanArtificialIntelligence-PoweredChatbottoSupportProfessionalManagementofDentalTrauma.JournalofEndodontics(2025).\\n712\\n[4] TianlongChen,XuxiChen,XianzhiDu,AbdullahRashwan,FanYang,HuizhongChen,ZhangyangWang,andYeqingLi.2023. Adamv-moe:\\n713\\nAdaptivemulti-taskvisionmixture-of-experts.InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision.17346\\u201317357.\\n714 [5] ZhangquanChen,ChunjiangLiu,andHaobinDuan.2024.AThree-Phases-LORAFinetunedHybridLLMIntegratedwithStrongPriorModulein\\n715 theEducationContext.InInternationalConferenceonArtificialNeuralNetworks.Springer,235\\u2013250.\\n716 [6] SusmitaDas,AmaraTariq,ThiagoSantos,SaiSandeepKantareddy,andImonBanerjee.2023.Recurrentneuralnetworks(RNNs):architectures,\\n717 trainingtricks,andintroductiontoinfluentialresearch.MachinelearningforBraindisorders(2023),117\\u2013138.\\n718 [7] TimDettmers,ArtidoroPagnoni,AriHoltzman,andLukeZettlemoyer.2023.Qlora:Efficientfinetuningofquantizedllms.Advancesinneural\\n719 informationprocessingsystems36(2023),10088\\u201310115.\\n[8] MaDongbo,SamiMiniaoui,LiFen,SaraAAlthubiti,andTheyabRAlsenani.2023.Intelligentchatbotinteractionsystemcapableforsentimental\\n720\\nanalysisusinghybridmachinelearningalgorithms.InformationProcessing&Management60,5(2023),103440.\\n721\\n[9] FilippoFlorindi,PasqualeFedele,andGiovannaMariaDimitri.2024. Anovelsolutionforthedevelopmentofasentimentalanalysischatbot\\n722\\nintegratingChatGPT.PersonalandUbiquitousComputing28,6(2024),947\\u2013960.\\n723\\n[10] DayaGuo,DejianYang,HaoweiZhang,JunxiaoSong,RuoyuZhang,RunxinXu,QihaoZhu,ShirongMa,PeiyiWang,XiaoBi,etal.2025.\\n724 Deepseek-r1:Incentivizingreasoningcapabilityinllmsviareinforcementlearning.arXivpreprintarXiv:2501.12948(2025).\\n725 [11] JiaaoHe,JidongZhai,TiagoAntunes,HaojieWang,FuwenLuo,ShangfengShi,andQinLi.2022.Fastermoe:modelingandoptimizingtrainingof\\n726 large-scaledynamicpre-trainedmodels.InProceedingsofthe27thACMSIGPLANSymposiumonPrinciplesandPracticeofParallelProgramming.\\n727 120\\u2013134.\\n728 ManuscriptsubmittedtoACM\\nDentalGPT:AMulti-ExpertTransformerModelforDentalInquiryResolution 15\\n729 [12] JiwooHong,NoahLee,andJamesThorne.2024.Orpo:Monolithicpreferenceoptimizationwithoutreferencemodel.arXivpreprintarXiv:2403.07691\\n730 (2024).\\n731 [13] JingchengHu,YinminZhang,QiHan,DaxinJiang,XiangyuZhang,andHeung-YeungShum.2025.Open-reasoner-zero:Anopensourceapproach\\n732 toscalingupreinforcementlearningonthebasemodel.arXivpreprintarXiv:2503.24290(2025).\\n[14] BahaIhnaini,YawenHuang,LianglinLi,JiayiWei,andShengyiQi.2024.EnhancingChineseMedicalDiagnosticChatbotthroughSupervised\\n733\\nFine-TuningofLargeLanguageModels.In20246thInternationalConferenceonInternetofThings,AutomationandArtificialIntelligence(IoTAAI).\\n734\\nIEEE,205\\u2013212.\\n735\\n[15] RaisaIslamandOwanaMarziaMoushi.2024.Gpt-4o:Thecutting-edgeadvancementinmultimodalllm.AuthoreaPreprints(2024).\\n736\\n[16] AaronJaech,AdamKalai,AdamLerer,AdamRichardson,AhmedEl-Kishky,AidenLow,AlecHelyar,AleksanderMadry,AlexBeutel,AlexCarney,\\n737 etal.2024.Openaio1systemcard.arXivpreprintarXiv:2412.16720(2024).\\n738 [17] KatikapalliSubramanyamKalyan.2024. AsurveyofGPT-3familylargelanguagemodelsincludingChatGPTandGPT-4. NaturalLanguage\\n739 ProcessingJournal6(2024),100048.\\n740 [18] ZeynepKarkiner,BegumYaman,BegumZengin,FerideNursenaCavli,andMustafaSert.2024.ParsyBot:chatbotforbaskentuniversityrelated\\n741 FAQs.In2024IEEE18thInternationalConferenceonSemanticComputing(ICSC).IEEE,168\\u2013175.\\n742 [19] FerielKhennouche,YoussefElmir,NabilDjebari,LarbiBoubchir,AbdelkaderLaouid,andAhceneBounceur.2024. ComparativeAnalysisand\\nApplicationofLargeLanguageModelsonFAQChatbots.In2024InternationalConferenceonComputationalIntelligenceandNetworkSystems(CINS).\\n743\\nIEEE,1\\u20136.\\n744\\n[20] PranjalKumar.2024.Largelanguagemodels(LLMs):survey,technicalframeworks,andfuturechallenges.ArtificialIntelligenceReview57,10(2024),\\n745\\n260.\\n746\\n[21] FatimaEzzahraLaghrissi,SamiraDouzi,KhadijaDouzi,andBadrHssina.2021.Intrusiondetectionsystemsusinglongshort-termmemory(LSTM).\\n747 JournalofBigData8,1(2021),65.\\n748 [22] AixinLiu,BeiFeng,BingXue,BingxuanWang,BochaoWu,ChengdaLu,ChenggangZhao,ChengqiDeng,ChenyuZhang,ChongRuan,etal.2024.\\n749 Deepseek-v3technicalreport.arXivpreprintarXiv:2412.19437(2024).\\n750 [23] ChengqiLyu,SongyangGao,YuzheGu,WenweiZhang,JianfeiGao,KuikunLiu,ZiyiWang,ShuaibinLi,QianZhao,HaianHuang,etal.2025.\\n751 Exploringthelimitofoutcomerewardforlearningmathematicalreasoning.arXivpreprintarXiv:2502.06781(2025).\\n752 [24] JunyanQiuandYipingYang.2024.TrainingLargeLanguageModelstoFollowSystemPromptwithSelf-SupervisedFine-tuning.In2024International\\nJointConferenceonNeuralNetworks(IJCNN).IEEE,1\\u20138.\\n753\\n[25] KRajasekaran,JohnAmose,GPreethika,SSangamithrra,andGGayathiri.2024.InnovationsinDentalCare:Chatbot-DrivenEfficiency.In2024\\n754\\n10thInternationalConferenceonAdvancedComputingandCommunicationSystems(ICACCS),Vol.1.IEEE,852\\u2013857.\\n755\\n[26] SamyamRajbhandari,ConglongLi,ZheweiYao,MinjiaZhang,RezaYazdaniAminabadi,AmmarAhmadAwan,JeffRasley,andYuxiongHe.2022.\\n756\\nDeepspeed-moe:Advancingmixture-of-expertsinferenceandtrainingtopowernext-generationaiscale.InInternationalconferenceonmachine\\n757 learning.PMLR,18332\\u201318346.\\n758 [27] KethireddyMaheedharReddyandRadhaGuha.2023.Automatictextsummarizationforconversationalchatbot.In2023IEEE8thInternational\\n759 ConferenceforConvergenceinTechnology(I2CT).IEEE,1\\u20137.\\n760 [28] FillipeBarrosRodrigues,WilliamFerreiraGiozza,RobsondeOliveiraAlbuquerque,andLuisJavierGarc\\u00edaVillalba.2022. Naturallanguage\\n761 processingappliedtoforensicsinformationextractionwithtransformersandgraphvisualization.IEEETransactionsonComputationalSocialSystems\\n762 (2022).\\n[29] DafneItzelRojasGonz\\u00e1lez,JosueAaronSorianoRivero,andJatziriHernandezHernandez.2024.WrapYourMindAroundEducation:Applying\\n763\\nHuggingFacetoaChatbotwithAI.InInternationalConferenceonDisruptiveTechnologies,TechEthicsandArtificialIntelligence.Springer,444\\u2013454.\\n764\\n[30] AbdullahiBSaka,LukumonOOyedele,LukmanAAkanbi,SikiruAGaniyu,DanielWMChan,andSururahABello.2023.Conversationalartificial\\n765\\nintelligenceintheAECindustry:Areviewofpresentstatus,challengesandopportunities.AdvancedEngineeringInformatics55(2023),101869.\\n766\\n[31] SuryaniandMustakim.2024. AnIntelligentChatbotforFacultyAdministrationUsingBidirectionalLSTMandSeq2SeqArchitecture.In2024\\n767 InternationalConferenceonSmartComputing,IoTandMachineLearning(SIML).IEEE,226\\u2013231.\\n768 [32] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,Timoth\\u00e9eLacroix,BaptisteRozi\\u00e8re,NamanGoyal,Eric\\n769 Hambro,FaisalAzhar,etal.2023.Llama:Openandefficientfoundationlanguagemodels.arXivpreprintarXiv:2302.13971(2023).\\n770 [33] UdayVallabhaneni,YatishWutla,TribhanginDichpally,VenkataRamiReddyCh,MeghamshReddyGone,andPLalithaKumari.2024.MiningMate:\\n771 AChatBotforNavigatingMiningRegulationsUsingLLMModels.In202410thInternationalConferenceonAdvancedComputingandCommunication\\n772 Systems(ICACCS),Vol.1.IEEE,888\\u2013892.\\n[34] LupingWang,ShengChen,LinnanJiang,ShuPan,RunzeCai,SenYang,andFeiYang.2025.Parameter-efficientfine-tuninginlargelanguage\\n773\\nmodels:asurveyofmethodologies.ArtificialIntelligenceReview58,8(2025),227.\\n774\\n[35] Sheng-KaiWang,Wan-LinYou,andShang-PinMa.2022.Semi-automaticChatbotGenerationforWebAPIs.InInternationalComputerSymposium.\\n775\\nSpringer,267\\u2013278.\\n776\\n[36] YuboWang,XueguangMa,GeZhang,YuanshengNi,AbhranilChandra,ShiguangGuo,WeimingRen,AaranArulraj,XuanHe,ZiyanJiang,etal.\\n777 2024.Mmlu-pro:Amorerobustandchallengingmulti-tasklanguageunderstandingbenchmark.AdvancesinNeuralInformationProcessingSystems\\n778 37(2024),95266\\u201395290.\\n779\\n780 ManuscriptsubmittedtoACM\\n16 DatTranetal.\\n781 [37] QingXia,HaotianZhao,andMingLiu.2024.PromptEngineeringApproachStudyforSupervisedFine-Tuned(SFT)LargeLanguageModels(LLMs)\\n782 inSpacecraftFaultDiagnosis.In20243rdConferenceonFullyActuatedSystemTheoryandApplications(FASTA).IEEE,819\\u2013824.\\n783 [38] SukwonYun,InyoungChoi,JiePeng,YangfanWu,JingxuanBao,QiyiwenZhang,JiayiXin,QiLong,andTianlongChen.2024.Flex-moe:Modeling\\n784 arbitrarymodalitycombinationviatheflexiblemixture-of-experts.AdvancesinNeuralInformationProcessingSystems37(2024),98782\\u201398805.\\n[39] ChenggangZhao,ChengqiDeng,ChongRuan,DamaiDai,HuazuoGao,JiashiLi,LiyueZhang,PanpanHuang,ShangyanZhou,ShirongMa,etal.\\n785\\n2025.Insightsintodeepseek-v3:Scalingchallengesandreflectionsonhardwareforaiarchitectures.InProceedingsofthe52ndAnnualInternational\\n786\\nSymposiumonComputerArchitecture.1731\\u20131745.\\n787\\n[40] ShiyongZheng,ZahrahYahya,LeiWang,RuihangZhang,andAzadehNooriHoshyar.2023.Multiheadeddeeplearningchatbotforincreasing\\n788\\nproductionandmarketing.InformationProcessing&Management60,5(2023),103446.\\n789\\n790\\nReceived20February2007;revised12March2009;accepted5June2009\\n791\\n792\\n793\\n794\\n795\\n796\\n797\\n798\\n799\\n800\\n801\\n802\\n803\\n804\\n805\\n806\\n807\\n808\\n809\\n810\\n811\\n812\\n813\\n814\\n815\\n816\\n817\\n818\\n819\\n820\\n821\\n822\\n823\\n824\\n825\\n826\\n827\\n828\\n829\\n830\\n831\\n832 ManuscriptsubmittedtoACM\\n\"\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import json\n",
    "import logging\n",
    "\n",
    "# Tắt cảnh báo CropBox\n",
    "logging.getLogger(\"pdfminer\").setLevel(logging.ERROR)\n",
    "\n",
    "def pdf_to_dumped_text(path):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text + \"\\n\"\n",
    "    return json.dumps(text)\n",
    "\n",
    "# Dùng thử\n",
    "pdf_path = \"TALLIP_Journal___Chat_Dental.pdf\"\n",
    "dumped_text = pdf_to_dumped_text(pdf_path)\n",
    "print(dumped_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
